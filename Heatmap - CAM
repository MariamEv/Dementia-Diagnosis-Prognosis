{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":861496,"sourceType":"datasetVersion","datasetId":457093}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Task: Improve Accuracy\n\nThe task is to improve the accuracy from 90% to 98-99%. Densenet161 gives 90% accuracy. The task is to optimize for accuracy but I will also show the F Beta score of the model as well. This notebook will feature a vgg16 model, trained in fastAI, and using progressive resizing and cutout to attain better results. Updated for september, I added some additional visuals to help understand the training and added a new section at the end for practical applications in healthcare.\n\nThe statement below was from the original 'completed' version of this notebook about 5 months ago. I am thrilled to say that this is no longer the case as an academic team improved upon my work to achieve truly phonomenal results\n`\nThe model only manages to achieve 95% accuracy, so I would consider this model a large step in the right direction, but not quite reaching the destination. Feel free to fork this notebook and play around, maybe you will find the missing link to reach the 98%+ mark. Check my final thoughts section for ideas for further improvement\n`\nThe current best performing model is now getting 99% on a custom test set, I highly recommend checking it out in the link to the paper in the special thanks section.\n\nSpecial Thanks to:\n* Sarvesh Dubey for both the dataset and the Task https://www.kaggle.com/tourist55\n* Zachary Burns, Derrick Cosmas, and Bryce Smith for taking the project I started here and improving upon it (I think its my first time being cited in an acedemic paper) http://noiselab.ucsd.edu/ECE228/projects/Report/52Report.pdf","metadata":{}},{"cell_type":"markdown","source":"<a id=\"TOC\"></a>\n## Table of Contents\n\n* [Part 2: Pint sized model](#part-two) \n* [part 3: Full sized fun](#part-three) \n    * [Potholes in high dimensional space](#pihds) \n* [Part 4: Test sets in FastAI](#part-four) \n    * [What does the model see](#wdtms) \n* [Conclusions](#Conclusions) \n\n\n# Part one: feeding in the data","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\nfrom fastai import *\nfrom fastai.vision import *","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2024-08-11T21:57:13.033886Z","iopub.execute_input":"2024-08-11T21:57:13.037201Z","iopub.status.idle":"2024-08-11T21:57:13.542344Z","shell.execute_reply.started":"2024-08-11T21:57:13.037153Z","shell.execute_reply":"2024-08-11T21:57:13.541342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This project uses FastAI version 1.0.60, the new version have several syntax differences and new features.","metadata":{}},{"cell_type":"code","source":"import fastai\nprint('The version of FastAI being used is:',fastai.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-08-11T21:57:16.472999Z","iopub.execute_input":"2024-08-11T21:57:16.473481Z","iopub.status.idle":"2024-08-11T21:57:16.498114Z","shell.execute_reply.started":"2024-08-11T21:57:16.473450Z","shell.execute_reply":"2024-08-11T21:57:16.496783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom fastai.vision.all import *\n\n# Assuming the Path function is also needed for file path handling\nfrom pathlib import Path\n\nimg = load_image(Path('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/train/VeryMildDemented/verymildDem972.jpg'))\nprint(img.shape)\nimg\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T21:57:18.541706Z","iopub.execute_input":"2024-08-11T21:57:18.542639Z","iopub.status.idle":"2024-08-11T21:57:27.007290Z","shell.execute_reply.started":"2024-08-11T21:57:18.542596Z","shell.execute_reply":"2024-08-11T21:57:27.006237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = Path('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T22:02:30.587156Z","iopub.execute_input":"2024-08-11T22:02:30.587562Z","iopub.status.idle":"2024-08-11T22:02:30.648486Z","shell.execute_reply.started":"2024-08-11T22:02:30.587531Z","shell.execute_reply":"2024-08-11T22:02:30.647344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cutout\nCutout is a method for removing small chunks of an image at random (see the batch images below to get an idea). This was one of the transforms that I experimented the most with and it helped close the gap a little between validation accuracy and test accuracy. ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"part-two\"></a>\n### Part 2: Pint sized model\n\nProgressive Resizing involves pretraining a model on shrunken versions of images (in this case, down to size 112x112) and then retraining the model on the full sized images (or possibly a few intermediate sized images in between if we want to get excessive).\n\n[Back to table of contents](#TOC) ","metadata":{}},{"cell_type":"code","source":"from fastai.vision.all import *\n\n# Define your transformations\ntransform = aug_transforms(\n    max_rotate=7.5,\n    max_zoom=1.15,\n    max_lighting=0.15,\n    max_warp=0.15,\n    p_affine=0.8,\n    p_lighting=0.8,\n    pad_mode='zeros'\n)\n\n# Example of loading data with transformations\npath = Path('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/train/VeryMildDemented/')\n\ndls = ImageDataLoaders.from_folder(\n    path,\n    valid_pct=0.2,\n    seed=42,\n    item_tfms=Resize(224),\n    batch_tfms=transform\n)\n\n# Display one batch of images\ndls.show_batch(max_n=4)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T22:02:33.995826Z","iopub.execute_input":"2024-08-11T22:02:33.996212Z","iopub.status.idle":"2024-08-11T22:02:36.904834Z","shell.execute_reply.started":"2024-08-11T22:02:33.996182Z","shell.execute_reply":"2024-08-11T22:02:36.903867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fastai.vision.all import *\n\n# Define your transformations including normalization\ntransform = aug_transforms(\n    max_rotate=7.5,\n    max_zoom=1.15,\n    max_lighting=0.15,\n    max_warp=0.15,\n    p_affine=0.8,\n    p_lighting=0.8,\n    pad_mode='zeros'\n)\n\n# Assuming PATH is defined and points to the correct dataset directory\nPATH = Path('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/')\n\ndata = ImageDataLoaders.from_folder(\n    PATH,\n    train='train',\n    valid_pct=0.4,\n    item_tfms=Resize(112),\n    batch_tfms=[*transform, Normalize.from_stats(*imagenet_stats)],\n    bs=64\n)\n\n# Display one batch of images\ndata.show_batch(max_n=6)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T22:02:40.265321Z","iopub.execute_input":"2024-08-11T22:02:40.265731Z","iopub.status.idle":"2024-08-11T22:02:43.458045Z","shell.execute_reply.started":"2024-08-11T22:02:40.265692Z","shell.execute_reply":"2024-08-11T22:02:43.457085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below is a nifty function to figure out the number of cases per class, written by a fellow by the name of James Briggs.\n\ncopy pasted from here: https://forums.fast.ai/t/get-value-counts-from-a-imagedatabunch/38784","metadata":{}},{"cell_type":"code","source":"# Get labels directly from the DataLoader\ntrain_labels = [label for _, label in data.train]\nlabel_counts = Counter(train_labels)\n\n\n#print(label_counts)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T22:02:46.639687Z","iopub.execute_input":"2024-08-11T22:02:46.640091Z","iopub.status.idle":"2024-08-11T22:02:55.082826Z","shell.execute_reply.started":"2024-08-11T22:02:46.640060Z","shell.execute_reply":"2024-08-11T22:02:55.081537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I explored many different models and found VGG16 to have more consistent performance. Other similar performing models were resnet50, resnet101, resnet152, densenet161 and vgg19_bn. I experimented with squeezenet but the performance was awful. I had also tried googlenet and inception v3, but these methods I was unable to make compatible with progressive resizing. ","metadata":{}},{"cell_type":"code","source":"from fastai.vision.all import *\nimport torch.nn as nn\n\nlearn = vision_learner(data, models.vgg16_bn, metrics=[FBeta(beta=1, average='weighted'), accuracy], wd=1e-1, cbs=ShowGraphCallback())\nlearn.fit_one_cycle(4)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T22:02:58.193250Z","iopub.execute_input":"2024-08-11T22:02:58.193652Z","iopub.status.idle":"2024-08-11T22:04:03.431430Z","shell.execute_reply.started":"2024-08-11T22:02:58.193619Z","shell.execute_reply":"2024-08-11T22:04:03.430338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**When to give more weight to precision**:\n1- False Positives are costly:\n* If the cost of false positives is high, --> give more weight to precision. ei. spam detection, marking a legitimate email as spam (false positive) can be very disruptive for users.\n2- High Confidence in Positive Predictions:\n* When you need to be very confident about the positive predictions. For example, in fraud detection, you might want to minimize the number of legitimate transactions flagged as fraudulent.\n**When to give more weight to recall:**\n1- False Negatives are costly:\n* If the cost of false negatives is high, you should give more weight to recall. For example, in medical diagnosis, missing a disease (false negative) can be very dangerous for the patient.\nEnsuring Detection of All Positive Cases:\n2- When it is critical to detect all positive cases. For example, in a search and rescue operation, you want to make sure you identify all possible locations where someone might be in danger, even if it means investigating some false leads.\nExamples: Spam Detection: More Precision: If you give more weight to precision, you reduce the chances of marking legitimate emails as spam, but you might miss some spam emails. More Recall: If you give more weight to recall, you ensure that most spam emails are caught, but you might also mark more legitimate emails as spam.\n> Medical Diagnosis: More Precision: If you give more weight to precision, you ensure that most of the diagnosed cases are actually positive, but you might miss some patients who have the disease. More Recall: If you give more weight to recall, you ensure that most patients with the disease are diagnosed, but you might have more false alarms.\n\n**Adjusting beta:**\nbeta < 1: This gives more weight to precision. Use this when false positives are more problematic.\nbeta > 1: This gives more weight to recall. Use this when false negatives are more problematic.\nbeta = 1: This balances precision and recall equally (F1 score). Use this when both false positives and false negatives are equally important.\n**Practical Considerations:**\nBusiness Impact: Evaluate the business impact of false positives and false negatives.\nUser Experience: Consider the user experience implications of your model's errors.\nRegulatory and Compliance: In some industries, regulatory requirements might dictate the importance of precision vs. recall.","metadata":{}},{"cell_type":"code","source":"Model_Path = Path('/kaggle/working/Alzheimer-stage-classifier-model/')\nlearn.model_dir = Model_Path\nlearn.save('checkpoint-1')","metadata":{"execution":{"iopub.status.busy":"2024-08-11T22:06:26.884304Z","iopub.execute_input":"2024-08-11T22:06:26.884769Z","iopub.status.idle":"2024-08-11T22:06:27.142255Z","shell.execute_reply.started":"2024-08-11T22:06:26.884736Z","shell.execute_reply":"2024-08-11T22:06:27.141116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fastai.vision.all import *\n\n# Assuming you already have your data and model setup\nlearn = vision_learner(data, models.vgg16_bn, metrics=[FBeta(beta=1, average='weighted'), accuracy], wd=1e-1, cbs=ShowGraphCallback())\nlearn.fit_one_cycle(4)\n\n# Plot the losses\nlearn.recorder.plot_loss()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T22:06:32.190415Z","iopub.execute_input":"2024-08-11T22:06:32.190816Z","iopub.status.idle":"2024-08-11T22:07:29.629234Z","shell.execute_reply.started":"2024-08-11T22:06:32.190787Z","shell.execute_reply":"2024-08-11T22:07:29.627969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Model_Path = Path('/kaggle/working/Alzheimer-stage-classifier-model/')\nlearn.model_dir = Model_Path\nlearn.save('checkpoint-1')","metadata":{"execution":{"iopub.status.busy":"2024-08-11T22:08:00.255410Z","iopub.execute_input":"2024-08-11T22:08:00.255823Z","iopub.status.idle":"2024-08-11T22:08:00.494758Z","shell.execute_reply.started":"2024-08-11T22:08:00.255792Z","shell.execute_reply":"2024-08-11T22:08:00.493783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the checkpoint\nlearn.load('checkpoint-1')\n\n# Plot the losses\nlearn.recorder.plot_loss()","metadata":{"execution":{"iopub.status.busy":"2024-08-11T22:08:03.120592Z","iopub.execute_input":"2024-08-11T22:08:03.121530Z","iopub.status.idle":"2024-08-11T22:08:03.633233Z","shell.execute_reply.started":"2024-08-11T22:08:03.121489Z","shell.execute_reply":"2024-08-11T22:08:03.632124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.load('checkpoint-1');\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T22:08:06.925521Z","iopub.execute_input":"2024-08-11T22:08:06.926418Z","iopub.status.idle":"2024-08-11T22:08:07.052805Z","shell.execute_reply.started":"2024-08-11T22:08:06.926375Z","shell.execute_reply":"2024-08-11T22:08:07.051765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.load('checkpoint-1')\n\nlearn.unfreeze()\n# Update the learning rate parameter to lr_max\nlearn.fit_one_cycle(4, lr_max=slice(1e-6, 3e-4))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T22:08:09.333078Z","iopub.execute_input":"2024-08-11T22:08:09.333452Z","iopub.status.idle":"2024-08-11T22:09:13.967992Z","shell.execute_reply.started":"2024-08-11T22:08:09.333424Z","shell.execute_reply":"2024-08-11T22:09:13.966971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fastai.vision.all import *\n\n# Load your model\nlearn.load('checkpoint-1')\n\n# Plot the losses\nlearn.recorder.plot_loss()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T22:10:20.131271Z","iopub.execute_input":"2024-08-11T22:10:20.132076Z","iopub.status.idle":"2024-08-11T22:10:20.610103Z","shell.execute_reply.started":"2024-08-11T22:10:20.132043Z","shell.execute_reply":"2024-08-11T22:10:20.608850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.save('checkpoint-2')","metadata":{"execution":{"iopub.status.busy":"2024-08-11T22:10:23.972947Z","iopub.execute_input":"2024-08-11T22:10:23.973963Z","iopub.status.idle":"2024-08-11T22:10:24.145557Z","shell.execute_reply.started":"2024-08-11T22:10:23.973922Z","shell.execute_reply":"2024-08-11T22:10:24.144483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I'm relatively new to DeepLearning with FastAI, from my limited experience with Progressive Resizing, the smaller training phases are most useful for the final product when they are still a bit underfit to the dataset. Further experimentation with this could, and possibly should be done to confirm.\n<a id=\"part-three\"></a>\n## Part 3: Full sized fun!\n\nNow that there is a reasonably good model for 112x112 images, now I will train the model on full sized images. Since I am now creating a final model, I will include the F Beta score to get a more complete view of the models performance. I also will modify the cutout section of the transforms to upscale the cutout sizes. \n\n[Back to table of contents](#TOC) ","metadata":{}},{"cell_type":"code","source":"# To delete the learner object and free up resources\ndel learn\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T22:10:34.624579Z","iopub.execute_input":"2024-08-11T22:10:34.624986Z","iopub.status.idle":"2024-08-11T22:10:35.121058Z","shell.execute_reply.started":"2024-08-11T22:10:34.624955Z","shell.execute_reply":"2024-08-11T22:10:35.119561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fastai.vision.all import aug_transforms, Resize, ImageDataLoaders\n\n# Define the transformations using aug_transforms\ntransform = aug_transforms(\n    max_rotate=7.5,\n    max_zoom=1.15,\n    max_lighting=0.15,\n    max_warp=0.15,\n    p_affine=0.8,\n    p_lighting=0.8\n)\n\n# Load your data and apply the transformations\ndls = ImageDataLoaders.from_folder(PATH, item_tfms=Resize(256), batch_tfms=transform)\n\n# Display some augmented images\ndls.show_batch()\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T22:10:40.910859Z","iopub.execute_input":"2024-08-11T22:10:40.911291Z","iopub.status.idle":"2024-08-11T22:10:43.777013Z","shell.execute_reply.started":"2024-08-11T22:10:40.911260Z","shell.execute_reply":"2024-08-11T22:10:43.775956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fastai.vision.all import *\n\n# Define your transformations using aug_transforms and additional custom transforms\ntransform = aug_transforms(\n    max_rotate=7.5,\n    max_zoom=1.15,\n    max_lighting=0.15,\n    max_warp=0.15,\n    p_affine=0.8,\n    p_lighting=0.8,\n    pad_mode='zeros'\n)\n\n# Additional custom transformations\ncustom_transforms = [\n    Resize(112, method='pad', pad_mode='zeros'),\n    Warp(magnitude=(0.1, 0.1)),  # ensure magnitude is a tuple\n    RandomErasing(p=0.5, sl=0.02, sh=0.4, min_aspect=0.3, max_count=6)\n]\n\n# Combine aug_transforms and custom_transforms\nall_transforms = transform + custom_transforms\n\n# Assuming PATH is defined and points to the correct dataset directory\nPATH = Path('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/')\n\n# Create a DataLoader with the combined transformations\ndata = ImageDataLoaders.from_folder(\n    PATH,\n    train='train',\n    valid_pct=0.4,\n    item_tfms=Resize(112),\n    batch_tfms=all_transforms,\n    bs=64\n)\n\n# Display one batch of images\ndata.show_batch(max_n=4)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T22:10:48.108525Z","iopub.execute_input":"2024-08-11T22:10:48.109554Z","iopub.status.idle":"2024-08-11T22:10:49.638211Z","shell.execute_reply.started":"2024-08-11T22:10:48.109505Z","shell.execute_reply":"2024-08-11T22:10:49.636758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = ImageDataBunch.from_folder(PATH, train=\"train/\",\n#                                  valid=\"train/\",\n                                  test=\"test/\",\n                                  valid_pct=.2,\n                                  ds_tfms=transform,\n                                  size=224,bs=32, \n                                  ).normalize(imagenet_stats)","metadata":{"execution":{"iopub.status.busy":"2024-08-11T22:10:59.864544Z","iopub.execute_input":"2024-08-11T22:10:59.865013Z","iopub.status.idle":"2024-08-11T22:10:59.962590Z","shell.execute_reply.started":"2024-08-11T22:10:59.864971Z","shell.execute_reply":"2024-08-11T22:10:59.961047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fastai.vision.all import aug_transforms, Resize, ImageDataLoaders\n\n# Define the transformations using aug_transforms\ntransform = aug_transforms(\n    max_rotate=7.5,\n    max_zoom=1.15,\n    max_lighting=0.15,\n    max_warp=0.15,\n    p_affine=0.8,\n    p_lighting=0.8\n)\n\n# Load your data and apply the transformations\ndls = ImageDataLoaders.from_folder(PATH, item_tfms=Resize(256), batch_tfms=transform)\n\n\n# Display some augmented images\ndls.show_batch()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T22:11:06.337431Z","iopub.execute_input":"2024-08-11T22:11:06.337797Z","iopub.status.idle":"2024-08-11T22:11:08.583360Z","shell.execute_reply.started":"2024-08-11T22:11:06.337768Z","shell.execute_reply":"2024-08-11T22:11:08.582170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the learner, similar to the training phase\nlearn = vision_learner(data, models.vgg16_bn, metrics=[FBeta(beta=1, average='weighted'), accuracy], wd=1e-1, cbs=ShowGraphCallback())\n\n# Set the model directory (where the model was saved during training)\nModel_Path = Path('/kaggle/working/Alzheimer-stage-classifier-model/')\nlearn.model_dir = Model_Path\n\n# Load the pre-trained model checkpoint\nlearn.load('checkpoint-2')\n\n# Perform a forward pass and evaluate on the validation/test set\nlearn.validate()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T22:11:48.108571Z","iopub.execute_input":"2024-08-11T22:11:48.108985Z","iopub.status.idle":"2024-08-11T22:11:54.908822Z","shell.execute_reply.started":"2024-08-11T22:11:48.108947Z","shell.execute_reply":"2024-08-11T22:11:54.907614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the learner\nlearn = vision_learner(data, models.vgg16_bn, metrics=[FBeta(beta=1, average='weighted'), accuracy], wd=1e-1, cbs=ShowGraphCallback())\n\n# Optionally load a pre-trained model\nlearn.model_dir = Path('/kaggle/working/Alzheimer-stage-classifier-model/')\nlearn.load('checkpoint-2')\n\n# Find the optimal learning rate\nlearn.lr_find()\n\n# Plot the learning rate finder results\nlearn.recorder.plot_lr_find()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T22:12:01.170994Z","iopub.execute_input":"2024-08-11T22:12:01.172027Z","iopub.status.idle":"2024-08-11T22:12:18.815359Z","shell.execute_reply.started":"2024-08-11T22:12:01.171982Z","shell.execute_reply":"2024-08-11T22:12:18.814102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"pihds\"></a>\n# Potholes in high dimensional space:\n\nIf you look at the training epochs below you will notice that the model hit a series of local minima on the way towards the optimal weights. I experimented with many combinations of weight decays, training cycles, learning rates, fine tuning, and the combination below worked the best. Even with these parameters the model still fumbles and falls on its face a little as it tries to find the optimal point.You can get an idea of how bunpy a ride it is with the added visual below.\n\nSmaller learning rates kept getting stuck in local minima then overfitting, higher learning rates kept overshooting and getting worse without overfitting. Fine tuning the model by unfreezing it and retraining the early layers with a mild learning rate caused the error rate to go through the roof. Weight decay worked just fine at 1e-1, did some experimenting but the accuracy never got much higher than 80% with other weight decay settings. As a funny side note I tested this with multiple different models, and the learning rate curves optimal point stayed the same, but the steepness of the slope was different (the larger models had a flatter curve while the smaller models had a sharper one). ","metadata":{}},{"cell_type":"code","source":"learn.fit_one_cycle(17, max_lr=5e-4)","metadata":{"execution":{"iopub.status.busy":"2024-08-11T22:12:21.377493Z","iopub.execute_input":"2024-08-11T22:12:21.378612Z","iopub.status.idle":"2024-08-11T22:12:21.483499Z","shell.execute_reply.started":"2024-08-11T22:12:21.378565Z","shell.execute_reply":"2024-08-11T22:12:21.481989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.save('checkpoint-3')","metadata":{"execution":{"iopub.status.busy":"2024-08-11T22:12:23.885415Z","iopub.execute_input":"2024-08-11T22:12:23.885914Z","iopub.status.idle":"2024-08-11T22:12:24.064074Z","shell.execute_reply.started":"2024-08-11T22:12:23.885859Z","shell.execute_reply":"2024-08-11T22:12:24.063025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.model\nlearn.recorder.plot_losses()","metadata":{"execution":{"iopub.status.busy":"2024-08-11T22:12:27.134930Z","iopub.execute_input":"2024-08-11T22:12:27.135616Z","iopub.status.idle":"2024-08-11T22:12:28.022886Z","shell.execute_reply.started":"2024-08-11T22:12:27.135581Z","shell.execute_reply":"2024-08-11T22:12:28.021449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learn)\nlosses,idxs = interp.top_losses()\nlen(data.valid_ds)==len(losses)==len(idxs)\ninterp.plot_confusion_matrix(figsize=(8,8))","metadata":{"execution":{"iopub.status.busy":"2024-08-11T22:12:31.378459Z","iopub.execute_input":"2024-08-11T22:12:31.379435Z","iopub.status.idle":"2024-08-11T22:12:41.404123Z","shell.execute_reply.started":"2024-08-11T22:12:31.379390Z","shell.execute_reply":"2024-08-11T22:12:41.402768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"part-four\"></a>\n# Part 4: Test sets in FastAI\n\nFastAI's data bunch doesn't attempt to compare test data with any labels, it assumes that the test set is only there to be labelled. As such, the cell below allows me to test the dataset on the test set and view the results. The results are output in an n+1 size list, where n refers to the number of evaluation metrics the model is given. I'm not sure what the first entry in the list does, each of the following entries are the scores of the model, output in the order in which they are received.\n\n[Back to table of contents](#TOC) ","metadata":{}},{"cell_type":"code","source":"transforms = aug_transforms()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T22:26:59.738131Z","iopub.execute_input":"2024-08-11T22:26:59.738501Z","iopub.status.idle":"2024-08-11T22:26:59.800321Z","shell.execute_reply.started":"2024-08-11T22:26:59.738471Z","shell.execute_reply":"2024-08-11T22:26:59.799202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def random_seed(seed_value, use_cuda):\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu  vars\n    random.seed(seed_value) # Python\n    if use_cuda: \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False\nrandom_seed(42, True)","metadata":{"execution":{"iopub.status.busy":"2024-08-11T22:27:03.906073Z","iopub.execute_input":"2024-08-11T22:27:03.906505Z","iopub.status.idle":"2024-08-11T22:27:03.980192Z","shell.execute_reply.started":"2024-08-11T22:27:03.906471Z","shell.execute_reply":"2024-08-11T22:27:03.979149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fastai.vision.all import *\n\ndef random_seed(seed_value, use_cuda):\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    random.seed(seed_value)\n    if use_cuda: \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nrandom_seed(42, True)\n\n# Updated DataLoader setup using ImageDataLoaders\ndata_test = ImageDataLoaders.from_folder(PATH,\n                                         train=\"test/\",\n                                         valid_pct=.95,\n                                         item_tfms=Resize(224),\n                                         batch_tfms=[*aug_transforms(), Normalize.from_stats(*imagenet_stats)],\n                                         bs=32,\n                                         num_workers=0)\n\n# Use metrics during Learner creation\nlearn = cnn_learner(data_test, models.resnet34, metrics=[error_rate, FBeta(beta=1, average='weighted')])\n\n# Validate the model on the test set\nev = learn.validate(dl=data_test.train)\nprint('Results from test set \\tError rate:', float(ev[1]), '\\tF Beta Score: ', float(ev[2]))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T22:30:08.131339Z","iopub.execute_input":"2024-08-11T22:30:08.132321Z","iopub.status.idle":"2024-08-11T22:30:11.819108Z","shell.execute_reply.started":"2024-08-11T22:30:08.132279Z","shell.execute_reply":"2024-08-11T22:30:11.818078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"wdtms\"></a>\n### What does the model see?\nBelow is where the real meat of this notebook is. We have a function which shows what areas of the image contributed the most to the appropriate diagnostic. With this method, one can view an image, see what the algorithmn predicted and why. This can help humans involved in the diagnostic process get a second opinion or bring attention high risk areas.\n\nAs a side note, I find it interesting that for alzheimers free and early stage MRI images the areas that are most highlighted are around the ventricals (the tubes in the center) while the late stage images often have the outer regions highlighted (which appear to be less dense).","metadata":{}},{"cell_type":"code","source":"# The code below is slighty modified from https://www.kaggle.com/daisukelab/verifying-cnn-models-with-cam-and-etc-fast-ai\n# and that code was a hevily modified version of https://nbviewer.jupyter.org/github/fastai/course-v3/blob/master/nbs/dl1/lesson6-pets-more.ipynb\n\nfrom fastai.callbacks.hooks import *\n\ndef visualize_cnn_by_cam(learn, data_index):\n    x, _y = learn.data.valid_ds[data_index]\n    y = _y.data\n    if not isinstance(y, (list, np.ndarray)): # single label -> one hot encoding\n        y = np.eye(learn.data.valid_ds.c)[y]\n\n    m = learn.model.eval()\n    xb,_ = learn.data.one_item(x)\n    xb_im = Image(learn.data.denorm(xb)[0])\n    xb = xb.cuda()\n\n    def hooked_backward(cat):\n        with hook_output(m[0]) as hook_a: \n            with hook_output(m[0], grad=True) as hook_g:\n                preds = m(xb)\n                preds[0,int(cat)].backward()\n        return hook_a,hook_g\n    def show_heatmap(img, hm, label):\n        _,axs = plt.subplots(1, 2)\n        axs[0].set_title(label)\n        img.show(axs[0])\n        axs[1].set_title(label)\n        img.show(axs[1])\n        axs[1].imshow(hm, alpha=0.6, extent=(0,img.shape[1],img.shape[1],0),\n                      interpolation='bilinear', cmap='magma');\n        plt.show()\n\n    for y_i in np.where(y > 0)[0]:\n        hook_a,hook_g = hooked_backward(cat=y_i)\n        acts = hook_a.stored[0].cpu()\n        grad = hook_g.stored[0][0].cpu()\n        grad_chan = grad.mean(1).mean(1)\n        mult = (acts*grad_chan[...,None,None]).mean(0)\n        show_heatmap(img=xb_im, hm=mult, label=str(learn.data.valid_ds.y[data_index]))\n\n\nidx_list = [0,1,2,31,3,63, 142, 207]        \n#idx_list = range(200,220)\nfor idx in idx_list:# range(10):\n    visualize_cnn_by_cam(learn, idx)","metadata":{"execution":{"iopub.status.busy":"2024-08-11T22:31:31.495274Z","iopub.execute_input":"2024-08-11T22:31:31.496046Z","iopub.status.idle":"2024-08-11T22:31:31.602125Z","shell.execute_reply.started":"2024-08-11T22:31:31.496014Z","shell.execute_reply":"2024-08-11T22:31:31.600477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fastai.vision.all import *\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef visualize_cnn_by_cam(learn, data_index):\n    x, _y = learn.dls.valid_ds[data_index]\n    y = _y if isinstance(_y, TensorCategory) else TensorCategory(_y)\n\n    # Ensure the model is in evaluation mode\n    m = learn.model.eval()\n\n    # Prepare the image tensor\n    dl = learn.dls.test_dl([x])\n    xb = dl.one_batch()[0]  # Get the input batch (ignore labels)\n    xb_im = dl.decode_batch((xb,))[0][0]  # Decode the batch for visualization\n    xb = xb.cuda()\n\n    def hooked_backward(cat):\n        with hook_output(m[0]) as hook_a: \n            with hook_output(m[0], grad=True) as hook_g:\n                preds = m(xb)\n                preds[0, int(cat)].backward()\n        return hook_a, hook_g\n\n    def overlay_heatmap(img, hm, label):\n        # Ensure the heatmap is the same size as the image\n        hm_resized = F.interpolate(hm, size=(img.shape[1], img.shape[2]), mode='bilinear', align_corners=False)\n        hm_resized = hm_resized.squeeze().cpu().numpy()\n\n        # Normalize the heatmap between 0 and 1\n        hm_resized = (hm_resized - hm_resized.min()) / (hm_resized.max() - hm_resized.min())\n\n        # Display the image with the heatmap overlay\n        fig, ax = plt.subplots()\n        ax.imshow(img.permute(1, 2, 0).cpu().numpy(), cmap='gray')  # Show the original image in grayscale\n        ax.imshow(hm_resized, cmap='magma', alpha=0.5)  # Overlay the heatmap with adjusted transparency\n        plt.title(label)\n        plt.axis('off')\n        plt.show()\n\n    # Convert y to at least 1D array, and handle multiple possible classes\n    y_indices = np.atleast_1d(y.numpy()).nonzero()[0]\n    \n    for y_i in y_indices:\n        hook_a, hook_g = hooked_backward(cat=y_i)\n        acts = hook_a.stored[0].cpu()\n        grad = hook_g.stored[0][0].cpu()\n\n        # Calculate the channel-wise mean of the gradients\n        grad_chan = grad.mean(1).mean(1)\n\n        # Multiply the activations with the mean gradients and then average\n        mult = (acts * grad_chan[..., None, None]).mean(0)\n\n        # Normalize the heatmap to ensure it spans [0, 1]\n        mult = mult - mult.min()\n        mult = mult / mult.max()\n\n        # Convert heatmap to a tensor before resizing\n        mult_tensor = mult.unsqueeze(0).unsqueeze(0).to(xb.device)\n\n        # Overlay the heatmap on the image\n        overlay_heatmap(img=xb_im, hm=mult_tensor, label=str(learn.dls.vocab[y_i]))\n\n# Example indices to visualize\nidx_list = [0, 1, 2, 31, 3, 63, 142, 207]\n\nfor idx in idx_list:\n    visualize_cnn_by_cam(learn, idx)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T22:30:35.128655Z","iopub.execute_input":"2024-08-11T22:30:35.129148Z","iopub.status.idle":"2024-08-11T22:30:37.825308Z","shell.execute_reply.started":"2024-08-11T22:30:35.129109Z","shell.execute_reply":"2024-08-11T22:30:37.824144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fastai.vision.all import *\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef visualize_cnn_by_cam(learn, data_index):\n    x, _y = learn.dls.valid_ds[data_index]\n    y = _y if isinstance(_y, TensorCategory) else TensorCategory(_y)\n\n    # Ensure the model is in evaluation mode\n    m = learn.model.eval()\n\n    # Prepare the image tensor\n    dl = learn.dls.test_dl([x])\n    xb = dl.one_batch()[0]  # Get the input batch (ignore labels)\n    xb_im = dl.decode_batch((xb,))[0][0]  # Decode the batch for visualization\n    xb = xb.cuda()\n\n    def hooked_backward(cat):\n        with hook_output(m[0]) as hook_a:  # Capture activations from the first layer\n            with hook_output(m[0], grad=True) as hook_g:  # Capture gradients\n                preds = m(xb)\n                preds[0, int(cat)].backward()\n        return hook_a, hook_g\n\n    def overlay_heatmap(img, hm, label):\n        # Resize heatmap to match the image size\n        hm_resized = F.interpolate(hm, size=(img.shape[1], img.shape[2]), mode='bilinear', align_corners=False)\n        hm_resized = hm_resized.squeeze().cpu().numpy()\n\n        # Normalize the heatmap\n        hm_resized = (hm_resized - hm_resized.min()) / (hm_resized.max() - hm_resized.min())\n\n        # Overlay the heatmap on the image\n        plt.imshow(img.permute(1, 2, 0).cpu().numpy(), cmap='gray')\n        plt.imshow(hm_resized, cmap='magma', alpha=0.5)\n        plt.title(label)\n        plt.axis('off')\n        plt.show()\n\n    # Handle multiple possible classes\n    y_indices = np.atleast_1d(y.numpy()).nonzero()[0]\n    \n    for y_i in y_indices:\n        hook_a, hook_g = hooked_backward(cat=y_i)\n        acts = hook_a.stored[0].cpu()\n        grad = hook_g.stored[0][0].cpu()\n\n        # Calculate gradients and activations to form the heatmap\n        grad_chan = grad.mean(1).mean(1)\n        mult = (acts * grad_chan[..., None, None]).mean(0)\n\n        # Normalize and convert heatmap to tensor\n        mult_tensor = mult.unsqueeze(0).unsqueeze(0).to(xb.device)\n        mult_tensor = (mult_tensor - mult_tensor.min()) / (mult_tensor.max() - mult_tensor.min())\n\n        # Overlay the heatmap on the image\n        overlay_heatmap(img=xb_im, hm=mult_tensor, label=str(learn.dls.vocab[y_i]))\n\n# Example indices to visualize\nidx_list = [0, 1, 2, 31, 3, 63, 142, 207]\n\nfor idx in idx_list:\n    visualize_cnn_by_cam(learn, idx)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T22:30:58.778084Z","iopub.execute_input":"2024-08-11T22:30:58.778534Z","iopub.status.idle":"2024-08-11T22:31:01.318953Z","shell.execute_reply.started":"2024-08-11T22:30:58.778496Z","shell.execute_reply":"2024-08-11T22:31:01.317939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"conclusions\"></a>\n\n # Final Thoughts:\n\nThe goal of the Task was to hit an accuracy of 98% or greater, looks like the approach I chose to take just gets me to around 95% accuracy with an f_beta score of around .95 (Didn't set any random states so those number can be off by a percentage point or two when you rerun the notebook). The one nitpick about the model I have is that it will generate a small number of false negatives (the model would predict someone does not have alzheimers when they do).\n\nIn training and validation the model is able to get an accuracy score as high as 99% and an F_beta of around .98, however on test data the model is only able to achieve 95%. The final piece in getting this model's performance up is to focus on how to tone the overfitting down a smidge (In my personal experience, the best performing models are always slightly overfit). There is probably room for improvement with the transforms that I chose for the images. There may also be room for improvement by adding another resizing step or changing some of the dimensions in the current step. It's also possible that combining the results from this model with another would be necessary to close the gap for 98%.\n\nIn 3/21/2020, this was the most accurate model for this dataset that I'm aware of. Now this model isn't the most accurate, but the heatmap interptiability adds a lot of value to this project\n\nThis year I've challenged myself to complete one task on Kaggle per week, in order to develop a larger Data Science portfolio. If you found this notebook useful or interesting please give it an upvote. I'm always open to constructive feedback. If you have any questions, comments, concerns, or if you would like to collaborate on a future task of the week feel free to leave a comment here or message me directly. For past TOTW check out the link to my page on github for this ongoing project\nhttps://github.com/Neil-Kloper/Weekly-Kaggle-Task/wiki\n\n[Back to table of contents](#TOC) ","metadata":{}}]}